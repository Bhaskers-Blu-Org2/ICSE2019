---
title: "Productionalizing scoring and re-fitting with ML pipelines"
output: html_notebook
---
# Introduction

This notebook illustrates organizing a ML workflow of transformations and model fitting into a Pipeline that can be handed off from  Data Scientist to  Data Engineer. It is based on the [Sparklyr ML Pipelines documentation](https://spark.rstudio.com/guides/pipelines/).

## Outline
1. Connect to Spark
2. Load data into Spark's in-memory storage
3. Specify the pipeline
4. Split the data into training and test sets
5. Fit the pipeline on the training set
6. Score the test set
7. Check prediction metrics
8. Save the pipelines to disk - Hand-off from Data Scientist to Data Engineer
9. Re-load the fitted pipeline from disk
10. Use the fitted pipeline to score new data
11. Load the unfitted pipeline and fit on new training data

# Connect to Spark
```{r}
if (file.exists("/dbfs")) {
  library(SparkR)
  library(sparklyr)
  sparkR.session()
  sc <- spark_connect(method = "databricks")
} else {
  library(sparklyr)
  spark_install()
  sc <- spark_connect(master = "local")
}

# Install additional packages, if needed
list.of.packages <- c("nycflights13", "pROC")
new.packages <- list.of.packages[!(list.of.packages %in% installed.packages()[,"Package"])]
if(length(new.packages)) install.packages(new.packages)
```
# Load data into Spark's in-memory storage
```{r}
library(nycflights13)
library(dplyr)

spark_flights <- sdf_copy_to(sc, flights, overwrite = TRUE)
```
# Count rows
```{r}
count(spark_flights)
```
# Specify the dplyr (SQL) operations
```{r}
# Filter, transform, and select
df <- spark_flights %>%
  filter(!is.na(dep_delay)) %>%
  mutate(
    month = paste0("m", month),
    day = paste0("d", day)
  ) %>%
  select(dep_delay, sched_dep_time, month, day, distance) 

# Preview the transformed data
df

# View the generated SQL 
ft_dplyr_transformer(sc, df) %>%
  ml_param("statement")

```
# Compose the full pipeline
```{r}
flights_pipeline <- ml_pipeline(sc) %>%
  ft_dplyr_transformer(
    tbl = df
  ) %>%
  ft_binarizer(
    input_col = "dep_delay",
    output_col = "delayed",
    threshold = 15
  ) %>%
  ft_bucketizer(
    input_col = "sched_dep_time",
    output_col = "hours",
    splits = c(400, 800, 1200, 1600, 2000, 2400)
  )  %>%
  ft_r_formula(delayed ~ month + day + hours + distance) %>% 
  ml_logistic_regression(threshold = 0.25)

flights_pipeline
```
# Split the data into training and test sets
- Note: splitting randomly - do not use for forecasting
```{r}
partitioned_flights <- sdf_random_split(
  spark_flights,
  training = 0.01,
  testing = 0.01,
  rest = 0.98,
  seed = 123
)
```
# Fit the pipeline on the training set
```{r}
fitted_pipeline <- ml_fit(
  flights_pipeline,
  partitioned_flights$training
)

fitted_pipeline
```
# Score the pipeline on the test set
```{r}
pred_testing <- ml_transform(fitted_pipeline, partitioned_flights$testing)
pred_testing
```
# Check prediction metrics
```{r}
library(pROC)

# Collect predictions into R
pred_testing_df <- collect(pred_testing)

# Extract the known labels and probability predictions
labels <- pred_testing_df$delayed
probabilities <- sapply(pred_testing_df$probability, last)

# Compute metrics
rocobj_prob <- roc(labels, probabilities)

# Print summary and AUC
rocobj_prob

# Request full set of metrics
rocobj_prob_metrics <- coords(rocobj_prob, "best", ret=c("threshold", "specificity", "sensitivity", "accuracy", "tn", "tp", "fn", "fp", "npv", "ppv", "1-specificity", "1-sensitivity", "1-accuracy", "1-npv", "1-ppv", "precision", "recall"))

# Print full metrics
as.data.frame(rocobj_prob_metrics)
```
# Save the unfitted and fitted pipelines to disk
```{r}
ml_save(
  flights_pipeline,
  "flights_pipeline",
  overwrite = TRUE
)

ml_save(
  fitted_pipeline,
  "flights_model",
  overwrite = TRUE
)
```
# Re-load the fitted pipeline from disk
```{r}
reloaded_model <- ml_load(sc, "flights_model")
```
# Use the re-loaded fitted pipeline for scoring
```{r}
new_df <- spark_flights %>%
  filter(
    month == 7,
    day == 5
  )

pred <- ml_transform(reloaded_model, new_df)
pred
```
# Fit on new training data
```{r}
reloaded_pipeline <- ml_load(sc, "flights_pipeline")

new_model <-  ml_fit(reloaded_pipeline, sample_frac(spark_flights, 0.01))

new_model
```
