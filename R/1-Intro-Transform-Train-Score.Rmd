---
title: "Simple, scalable ML workflow"
output: html_notebook
---
# Introduction
This notebook is a simple example of training and scoring a machine learning model in Spark. It is based on the [Sparklyr MLlib documentation](https://spark.rstudio.com/mlib/).

## Outline
1. Connect to Spark
2. Load data into Spark's in-memory storage
3. Transform the data
4. Split the data into training and test sets
4. Fit an ML model on the training set
5. Use the fitted model to Score the test set

## A note on R Notebooks
This is an [R Markdown](http://rmarkdown.rstudio.com) Notebook. When you execute code within the notebook in RStudio, the results appear beneath the code. 

Try executing the following chunk by clicking the *Run* button within the chunk or by placing your cursor inside it and pressing *Ctrl+Shift+Enter*.

# Connect to Spark
```{r}
if (file.exists("/dbfs")) {
  # Connecting on a Databricks cluster
  library(SparkR)
  library(sparklyr)
  sparkR.session()
  sc <- spark_connect(method = "databricks")
} else {
  # Connecting on a local machine
  library(sparklyr)
  spark_install()
  sc <- spark_connect(master = "local")
}
```
# Copy the mtcars dataset into Spark's distributed in-memory storage
```{r}
mtcars_tbl <- copy_to(sc, mtcars, "mtcars", overwrite = TRUE)

# Print the class attribute of "mtcars", an R data.frame
class(mtcars)

# Print the class attribute of "mtcars_tbl", a handle to a Spark dataframe
class(mtcars_tbl)
```
# Transform the data with Spark SQL, feature transformers, and DataFrame functions.

- Use Spark SQL to remove all cars with horsepower less than 100
- Use Spark feature transformers to bucket cars into two groups based on cylinders
- Use Spark DataFrame functions to partition the data into test and training
```{r}
library(dplyr)

# transform our data set, and then partition into 'training', 'test'
partitions <- mtcars_tbl %>%
  filter(hp >= 100) %>%
  ft_bucketizer("cyl", "cyl8", splits = c(4, 8, 8.01)) %>%
  sdf_random_split(training = 0.5, test = 0.5, seed = 888)

# Print the class attribute of "partitions$training", a handle to a Spark dataframe
class(partitions$training)

# Count rows
count(partitions$training)
count(partitions$test)
```
# Spark SQL
```{r}
library(DBI)

# Register partitions$training as a Spark SQL table named "training_set"
partitions_training <- sdf_register(partitions$training, "training_set")

class(partitions_training)

dbGetQuery(sc, "SELECT COUNT(*) FROM training_set")
```
# Fit a linear model using spark ML
- Model MPG as a function of weight and cylinders
- What happens if we replace "cyl" with "cyl8"?
```{r}
# fit the model using Spark MLlib's linear regression function
fit <- partitions$training %>%
  ml_linear_regression(mpg ~ wt + cyl)

# print a summary of the fitted model
summary(fit)
```
# Score the data
- Use our fitted model to predict the label column (mpg) in the test set
- Collect the results from Spark into an R data.frame
```{r}
# Perform scoring and bring the results back to R
pred <- ml_predict(fit, partitions$test) %>%
  collect

# Print the class attribute of "pred"
class(pred)

# Calculate the Root Mean Squared Prediction Error
rmse_test <- sqrt(mean((pred$mpg - pred$prediction) ^ 2))
rmse_test

# Print the actual and predicted mpg
pred[, c("mpg", "prediction")]
```
# Plot the predictions
```{r}
library(ggplot2)

# Plot the predicted versus actual mpg
ggplot(pred, aes(x = mpg, y = prediction)) +
  geom_abline(lty = "dashed", col = "red") +
  geom_point() +
  theme(plot.title = element_text(hjust = 0.5)) +
  coord_fixed(ratio = 1) +
  labs(
    x = "Actual Fuel Consumption",
    y = "Predicted Fuel Consumption",
    title = "Predicted vs. Actual Fuel Consumption"
  )
```

