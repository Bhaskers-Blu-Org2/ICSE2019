---
title: "Productionalizing scoring and re-fitting with ML pipelines"
output: html_notebook
---
# Introduction

This notebook illustrates organizing a ML workflow of transformations and model fitting into a Pipeline that can be handed off from  Data Scientist to  Data Engineer. It is based on the [Sparklyr ML Pipelines documentation](https://spark.apache.org/docs/latest/ml-pipeline.html).

## Outline
1. Connect to Spark
2. Load data into Spark's in-memory storage
3. Specify the pipeline
4. Split the data into training and test sets
4. Fit the pipeline on the training set
5. Save the fitted pipeline to disk
6. Re-load the fitted pipeline from disk
7. Use the fitted pipeline to score new data
8. Re-fit the pipeline on new training data

# Connect to Spark
```{r}
if (file.exists("/dbfs")) {
  library(SparkR)
  library(sparklyr)
  sparkR.session()
  sc <- spark_connect(method = "databricks")
} else {
  library(sparklyr)
  spark_install()
  sc <- spark_connect(master = "local")
}

# Install additional packages, if needed
list.of.packages <- c("nycflights13", "pROC")
new.packages <- list.of.packages[!(list.of.packages %in% installed.packages()[,"Package"])]
if(length(new.packages)) install.packages(new.packages)
```
# Load data into Spark's in-memory storage
```{r}
library(nycflights13)
library(dplyr)

spark_flights <- sdf_copy_to(sc, flights, overwrite = TRUE)
```
# Count rows
```{r}
count(spark_flights)
```
# Specify the dplyr (SQL) operations
```{r}
# Filter, transform, and select
df <- spark_flights %>%
  filter(!is.na(dep_delay)) %>%
  mutate(
    month = paste0("m", month),
    day = paste0("d", day)
  ) %>%
  select(dep_delay, sched_dep_time, month, day, distance) 

# Preview the transformed data
df

# View the generated SQL 
ft_dplyr_transformer(sc, df) %>%
  ml_param("statement")

```
# Compose the full pipeline
```{r}
flights_pipeline <- ml_pipeline(sc) %>%
  ft_dplyr_transformer(
    tbl = df
  ) %>%
  ft_binarizer(
    input_col = "dep_delay",
    output_col = "delayed",
    threshold = 15
  ) %>%
  ft_bucketizer(
    input_col = "sched_dep_time",
    output_col = "hours",
    splits = c(400, 800, 1200, 1600, 2000, 2400)
  )  %>%
  ft_r_formula(delayed ~ month + day + hours + distance) %>% 
  ml_logistic_regression(threshold = 0.25)

flights_pipeline
```
# Split the data into training and test sets
```{r}
partitioned_flights <- sdf_random_split(
  spark_flights,
  training = 0.01,
  testing = 0.01,
  rest = 0.98,
  seed = 123
)
```
# Fit the pipeline on the training set
```{r}
fitted_pipeline <- ml_fit(
  flights_pipeline,
  partitioned_flights$training
)

fitted_pipeline
```
# Save the unfitted and fitted pipelines to disk
```{r}
ml_save(
  flights_pipeline,
  "flights_pipeline",
  overwrite = TRUE
)

ml_save(
  fitted_pipeline,
  "flights_model",
  overwrite = TRUE
)
```
# Re-load the fitted pipeline from disk
```{r}
reloaded_model <- ml_load(sc, "flights_model")
```
# Use the re-loaded fitted pipeline for scoring
```{r}
new_df <- spark_flights %>%
  filter(
    month == 7,
    day == 5
  )

pred <- ml_transform(reloaded_model, new_df)
pred
```
# Re-fit the pipeline on new training data
```{r}
reloaded_pipeline <- ml_load(sc, "flights_pipeline")

new_model <-  ml_fit(reloaded_pipeline, sample_frac(spark_flights, 0.01))

new_model
```
