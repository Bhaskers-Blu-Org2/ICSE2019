{
  "cells": [
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# Data Science Fundamentals in Python 3\n\n##  Introduction\n\nThe purpose of this example is to practice different components of within a data science process. We will use the classic 1974 *Motor Trend* car road tests (`mtcars`) dataset. \n\nWe are going to practice:\n1. How does a machine learn using Stochastic Gradient Descent\n2. How to load data into Pandas dataframe and explore. \n3. How to train different models using Scikit-learn library and compare their performances. \n    - A linear model using all variables\n    - A Gradient Boosting Machine (GBM) model "
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Section 1. Practice on Machine Learning using Stochastic Gradient Descent (SGD)\n\n<img src=\"http://rasbt.github.io/mlxtend/user_guide/general_concepts/gradient-optimization_files/ball.png\" width=\"600\" align=\"left\"/>"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### 1.1 Generate the data"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "import numpy as np\nimport pandas as pd\nMU, SIGMA = 6, 2\nSIGMA_NOISE = 0.05\nNUM_OBS = 100\nx = np.random.normal(MU, SIGMA, NUM_OBS)\nnoise = np.random.normal(0, SIGMA_NOISE, NUM_OBS)\nA, B = 3.5, 8.5\ny = A + B * x + noise",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### 1.2 Split the Data into Training and Testing"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=123)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### 1.3 Start Training the Model\n\nHere is the formula we used to derive the slopes for coefficients a and b. \n\n<img src=\"https://hzstorage.blob.core.windows.net/icse2019/sgd-derivatives.PNG?sv=2018-03-28&ss=bqtf&srt=sco&sp=rwdlacup&se=2019-05-23T23:59:54Z&sig=ETqhmBLQ4GuFiIQUAgqAKKScLtEQqAoyGUUwN8oI%2BUo%3D&_=1558627256972\" width=\"300\" align=\"left\"/>"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from random import shuffle\nNUM_EPOCHS = 40\nLEARNING_RATE = 0.01\nNUM_TRAINING_OBS = len(X_train)\na_hat, b_hat = np.random.normal(0, 1, 2)\nprint(a_hat, b_hat)\nsse_progress = [0] * NUM_EPOCHS\na_progress = [0] * NUM_EPOCHS\nb_progress = [0] * NUM_EPOCHS\ntrain_index = list(range(NUM_TRAINING_OBS))\nfor k in range(NUM_EPOCHS):\n    shuffle(train_index)\n    SSE = 0\n    for i in train_index:\n        y_hat = a_hat + b_hat * X_train[i]\n        delta = y_train[i] - y_hat\n        SSE += delta**2\n        slope_a = 2 * delta * (-1)\n        slope_b = 2 * delta * (-X_train[i])\n        a_hat = a_hat - slope_a * LEARNING_RATE\n        b_hat = b_hat - slope_b * LEARNING_RATE\n    sse_progress[k] = SSE\n    a_progress[k] = a_hat\n    b_progress[k] = b_hat\n    print(\"Epoch = {0}, SSE={1}\".format(k, round(SSE,4)))\nprint(\"In the end, the learned coefficients are {0} and {1}.\".format(round(a_hat, 4), round(b_hat, 4)))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### 1.4 Plot the Training Progress"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "%matplotlib inline\nimport matplotlib.pyplot as plt\nplt.figure(1)\nplt.subplot(3,1,1)\nplt.plot(range(1, NUM_EPOCHS+1), a_progress)\nplt.xlabel(\"Training Epochs\")\nplt.ylabel(\"Coefficient a\")\n\nplt.subplot(3,1,2)\nplt.plot(range(1, NUM_EPOCHS+1), b_progress)\nplt.xlabel(\"Training Epochs\")\nplt.ylabel(\"Coefficient b\")\n\nplt.subplot(3,1,3)\nplt.plot(range(2, NUM_EPOCHS+1), sse_progress[1:])\nplt.xlabel(\"Training Epochs\")\nplt.ylabel(\"SSE\")\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Section 2. Train Models Using Scikit-Learn Library"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### 2.1 Prepare Data\n\nWe'll start by loading the `mtcars` sample dataset and displaying its description:"
    },
    {
      "metadata": {
        "scrolled": true,
        "trusted": true
      },
      "cell_type": "code",
      "source": "!pip install pydataset --disable-pip-version-check -q  # install a Python package containing the dataset\nimport pydataset\nfrom pydataset import data\ndf = data('mtcars')\ndf.head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "We can also quickly examine the distribution of values and first few rows of the dataset:"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### 2.2 Explore the Data in a Better Detail using pandas-profiling Package"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "df.describe()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### 2.3 Get More Detailed Report of the Data Using Pandas Profiling"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "!pip install pandas-profiling",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "import pandas_profiling\n# Drop the row index of the data to avoid special characters in row index\ndf1 = df.reset_index(drop=True)\npandas_profiling.ProfileReport(df1)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### 2.4 Split the Data into Training and Testing"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "The goal for the machine learning models in this tutorial will be to predict each car's gas mileage (`mpg`) from the car's other features.\n\nWe will split the records into training and test datasets: each model will be fitted using the training data, and evaluated using the withheld test data."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# split the dataset into features available for prediction (X) and value to predict (y)\ny = df['mpg'].values\nX = df.drop('mpg', 1).values\nfeature_names = df.drop('mpg', 1).columns\n\n# save 30% of the records for the test set\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=123)\nX_train.shape",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "As you can see from the description above, the number of predictive features available in this dataset (10) is comparable to the number of records (22). Such conditions tend to produce overfitted models that give exceptional predictions on their own training data, but poor predictions on the withheld test data. We will see an example of an overfitted model below."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### 2.5 Fit Models\n#### 2.5.1 Linear Regression Model\nThe following lines of code fit a linear model (without regularization) using all of the original features:"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from sklearn.linear_model import LinearRegression\n\nlm = LinearRegression()\nlm.fit(X_train, y_train)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Below, we print the R-squared value for the true vs. predicted `mpg` values in the *training* set. We also show the fitted coefficients for different features."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "import pandas as pd\nfrom sklearn.metrics import r2_score\n\n# print R^2 for the training set\nprint('The R-squared value for the training set is: {:0.4f}'.format(r2_score(y_train, lm.predict(X_train))))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Notice that the model performs very well on the training data to which it was fitted. (Predictions of the model account for 89% of the variance in `mpg` values.) Some of the feature coefficients may reflect our intuition: for example, heavy cars tend to have worse gas mileage ($\\beta_{\\textrm{wt}} = -5.0$), and cars with manual transmissions tend to have better gas mileage ($\\beta_{\\textrm{am}} = 5.2$).\n\nNow, let's check the model's performance on the test dataset:"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "import numpy as np\n\npredicted = lm.predict(X_test)\n\nr_squared = r2_score(y_test, predicted)\nmae = np.mean(abs(predicted - y_test))\nrmse = np.sqrt(np.mean((predicted - y_test)**2))\nrae = np.mean(abs(predicted - y_test)) / np.mean(abs(y_test - np.mean(y_test)))\nrse = np.mean((predicted - y_test)**2) / np.mean((y_test - np.mean(y_test))**2)\n\n# Create a data frame for storing results from each model\nsummary_df = pd.DataFrame(index = ['R-squared', 'Mean Absolute Error', 'Root Mean Squared Error',\n                                   'Relative Absolute Error', 'Relative Squared Error'])\nsummary_df['Linear Regression, all variables'] = [r_squared, mae, rmse, rae, rse]\nsummary_df",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Notice that the R-squared value for true vs. predicted `mpg` of the test set is much lower than it was for the training set. (Granted, our test set is not very large, so some fluctuation is expected.) This is indicative of model overfitting."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Gradient Boosting Machine Regression Model"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from sklearn.ensemble import GradientBoostingRegressor\nfrom time import time\nparams = {'alpha':0.9, 'criterion': 'friedman_mse', 'learning_rate': 0.01,\n         'loss': 'ls', 'max_depth': 2, 'min_samples_leaf': 1, \n         'min_samples_split': 2, 'n_estimators': 200, 'random_state': 123, \n         'subsample': 1, 'verbose': 0}\nparams['random_state'] = 123\nparams['loss'] = 'ls'\ngbm = GradientBoostingRegressor(**params)\n\ngbm.fit(X_train, y_train)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Now we can check the model's performance on the test data:"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "predicted = gbm.predict(X_test)\n\nr_squared = r2_score(y_test, predicted)\nmae = np.mean(abs(predicted - y_test))\nrmse = np.sqrt(np.mean((predicted - y_test)**2))\nrae = np.mean(abs(predicted - y_test)) / np.mean(abs(y_test - np.mean(y_test)))\nrse = np.mean((predicted - y_test)**2) / np.mean((y_test - np.mean(y_test))**2)\n\nsummary_df['Gradient Boosted Machine Regression'] = [r_squared, mae, rmse, rae, rse]\nsummary_df",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "name": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}